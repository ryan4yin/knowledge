## 零、服务网格与 API 网关

Istio 服务网格自带的 API 网关功能比较有局限性，实际应用中还是需要结合第三方 API 网关，或者在 Envoy 上深度开发，才能满足需求。

关于 Istio 与 APISIX/Kong 等其他网关的集成，详见 [Ingress / API Gateway 与服务网格集成 {#api-gateway-plus-service-mesh](../../ingress-egress/README.md#api-gateway-plus-service-mesh)


## 一、控制数据面 proxy 的启动、终止顺序

### 预先启动 proxy，或者说在 proxy 启动后再启动其他容器

Istio 1.7 新增了一个功能: [delay the application start until after the sidecar is started](https://istio.io/latest/news/releases/1.7.x/announcing-1.7/change-notes/#traffic-management)

通过设置 operator 参数 `values.global.proxy.holdApplicationUntilProxyStarts=true`，就能全局启用此功能，让主容器在 Sidecar 就绪后启动，避免主容器因为网络未就绪而 Crash.

或者在 pod 上添加如下注解也是一样的效果（只在该 pod 上生效）：

```yaml
      annotations:
        # https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig
        proxy.istio.io/config: |
          holdApplicationUntilProxyStarts: true
```

建议：通常只有一启动就需要访问外部网络的服务，会有这个问题，因此我认为应该按需在每个 Pod 上启用此功能，没必要改全局配置。

### 为主容器及 Sidecar 均添加 preStop 参数

>https://github.com/hashicorp/consul-k8s/issues/650

首先，我在 [K8s 最佳实践](./../../最佳实践.md) 的「优雅停止（Gracful Shutdown）与 502/504 报错」详细说明过，为什么要为 Pod 添加 `preStop`.

在 Sidecar 代理了 Pod 流量的情况下，上述问题会变得更复杂一点——还需要考虑一个关闭顺序的问题：

- 如果在 Envoy 已关闭，有新的请求再进来，将会导致 504
  - 所以 Envoy 最好在 Terminating 至少 3s 后才能关，确保 Istio 网格配置已完全更新
- 如果在 Envoy 还没停止时，主容器先关闭，然后又有新的请求再进来，Envoy 将因为无法连接到 upstream 导致 503
  - 所以主容器也最好在 Terminating 至少 3s 后，才能关闭。
- 如果主容器处理还未处理完遗留请求时，Envoy 或者主容器的其中一个停止了，会因为 tcp 连接直接断开连接导致 502
  - 因此 Envoy 必须在主容器处理完遗留请求后（即没有 tcp 连接时），才能关闭

所以总结下：Envoy 及主容器的 `preStop` 都至少得设成 3s，并且在「没有 tcp 连接」时，才能关闭，避免出现 502/503/504.

主容器的修改方法在前文中已经写过了，下面介绍下 Envoy 的修改方法。

和主容器一样，Envoy 也能直接加 `preStop`，修改 `istio-sidecar-injector` 这个 `configmap`，在 sidecar 里添加 preStop sleep 命令:

```yaml
    containers:
    - name: istio-proxy
      # 添加下面这部分
      lifecycle:
      preStop:
          exec:
            command:
            - /bin/sh
            - -c
            - "while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done"
```

#### 无效的手段

直接设置 `terminationDrainDuration`，Envoy 会因为被终止，而在 Terminating 的一瞬间立即开始拒绝新的 TCP 连接，只等待旧连接关闭。
如果这时 Istio 网格配置还没完全更新，就可能遇到 tcp 连接被 Envoy 拒绝的情况。

如果客户端也有 Enovy 做正向代理，这个客户端 Envoy 可能会报错 503.

因此如下配置不适用：

```yaml
annotations:
  # https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig
  proxy.istio.io/config: |
    terminationDrainDuration: 30s
```

必须得通过 preStop 来延迟 Envoy 收到 SIGTERM 信号的时间，至少延迟个 3s 吧。


## [转发客户端 IP/证书](https://istio.io/latest/docs/ops/configuration/traffic-management/network-topologies/)


全局配置：

```yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    defaultConfig:
      gatewayTopology:
        numTrustedProxies: 2
        forwardClientCertDetails: APPEND_FORWARD  # 通常只在使用 mTLS 双向认证时，服务端才会要求转发证书
```

pod 级别的配置：

```yaml
...
  metadata:
    annotations:
      "proxy.istio.io/config": '{"gatewayTopology" : { "numTrustedProxies": 2, "forwardClientCertDetails": APPEND_FORWARD } }'
```



## 二、Pod 预热功能 - slow start

目前 Istio 仍然欠缺类似 [AWS ALB](https://aws.amazon.com/about-aws/whats-new/2018/05/application-load-balancer-announces-slow-start-support/)/[nginx](http://nginx.org/en/docs/http/ngx_http_upstream_module.html#slow_start) 的 `slow_start` 的能力，缓慢地将流量切给新建的 Pods，相关进展如下：

- Istio 1.11 目前还没有进展：[Ability to gradually warm services - Istio issues](https://github.com/istio/istio/issues/21228)
- Envoy 1.20+ 已支持 slow_start 模式：[Slow start mode - envoy](https://www.envoyproxy.io/docs/envoy/v1.20.0/intro/arch_overview/upstream/load_balancing/slow_start#arch-overview-load-balancing-slow-start)


应该可以通过 EnvoyFilter 提前用上这个能力，待研究

## 三、HPA 扩缩容

Pod 在添加 Sidecar 后，HPA 扩缩容的指标会受到影响。

参见 [Horizontal Pod Autoscaler - Pod 自动扩缩容](../../autoscaling/Horizontal%20Pod%20Autoscaler.md)


## 四、在特定 Pods 上启用 access log:


方法一：使用新的 Telemetry API
- https://github.com/istio/istio.io/issues/7613#issuecomment-1009693940

方法二：使用 EnvoyFilter:

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: enable-rem-st-accesslog
  namespace: prod
spec:
  workloadSelector:
    labels:
      app: rem-st
  configPatches:
  - applyTo: NETWORK_FILTER
    match:
      context: ANY
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
    patch:
      operation: MERGE
      value:
        typed_config:
          "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager"
          access_log:
          - name: envoy.access_loggers.file
            typed_config:
              "@type": type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog
              path: "/dev/stdout"
              format: "[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% \"%DYNAMIC_METADATA(istio.mixer:status)%\" \"%UPSTREAM_TRANSPORT_FAILURE_REASON%\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\" \"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\n"
```

## 五、直接返回响应 - Direct Response

>https://github.com/istio/istio/issues/29264

Istio 只支持 redirect，缺乏除 Redirect 之外直接返回响应的能力，所有的语义目前都只支持转发到某个上游 Workload，直接返回响应需要借助 EnvoyFilter 或其他手段实现。

方法一，是使用 EnvoyFilter，如下是一个直接在 IngressGateway 上面返回 200 的例子：

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: direct
spec:
  workloadSelector:
    labels:
      istio: ingressgateway
  configPatches:
  - applyTo: HTTP_ROUTE
    match:
      context: GATEWAY  # 网关层
      routeConfiguration:
        vhost:
          name: <domain_here>:443  # host 地址
    patch:
      operation: INSERT_FIRST
      value:
        name: direct
        match:
          path: /direct  # HTTP 路径
        directResponse:
          body:
            inlineString: 'hello world'  # 响应体
          status: 200  # 响应状态码
```

方法二呢，是专门在集群跑一个程序专门用来处理这类请求，比如直接定义几个处理方法:

- Path 为 `/direct_response/xxx` 或者 `Header` 包含 `direct_response: xxx`，直接返回状态码 xxx

这样就可以直接在 VirtualService 上通过 rewrite uri 或者设置 headers，然后转发到这个「Direct Response」服务来达到返回固定请求的目的。


## 六、Istio 去除响应 Headers

在结合使用 Nginx/APISIX/Kong 等自定义网关与 Istio 时，给它们注入 Sidecar 后会发现响应头中带了些 `x-envoy-` 开头的信息，这一是浪费流量，二是暴露出这些信息存在风险！

- 相关 issue
  - [Unable to remove server header](https://github.com/istio/istio/issues/13861)
  - [Strip internal mesh-machinery headers when sending requests/responses out of mesh](https://github.com/istio/istio/issues/17635)


总结下上述 Issue 的内容，目前的解决方法是部署如下 Envoyfilter 配置：

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: remove-envoy-response-headers
  namespace: my-test-gateway
spec:
  workloadSelector:
    labels:
      app: my-test-gateway
  configPatches:
  - applyTo: HTTP_ROUTE
    match:
      context: SIDECAR_INBOUND # this property fixes the ingress outbound
    patch:
      operation: MERGE
      value:
        decorator:  # remove header: x-envoy-decorator-operation
          propagate: false
  - applyTo: NETWORK_FILTER
    match:
      context: SIDECAR_INBOUND
      listener:
        filterChain:
          filter:
            name: "envoy.filters.network.http_connection_manager"
    patch:
      operation: MERGE
      value:
        typed_config:
          "@type": "type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager"
          server_header_transformation: PASS_THROUGH
  - applyTo: ROUTE_CONFIGURATION
    match:
      context: SIDECAR_INBOUND
    patch:
      operation: MERGE
      value:
        response_headers_to_remove:
        - "x-envoy-upstream-service-time"
        - "server"
```

## 七、Istio Sidecar 代理了不需要代理的流量导致性能差

Istio Sidecar 默认会代理 Pod 的 inbound 以及 outbound 两部分流量，但是有些场景下其实是不需要拦截的，比如：

- 如果使用了 APISIX/Kong 作为入口网关，其实它的 Istio Sidecar 不需要拦截 inbound 流量，APISIX 会处理好这些
- 如果服务直接访问公网域名，通常都会直接走 443/80 端口，这部分流量通常也不需要 istio 做额外处理

为 pod 添加 annotation，即可告诉 Istio 不要拦截  inbound/outbound 流量：

```yaml
      annotations:
        # https://istio.io/latest/docs/reference/config/annotations/
        traffic.sidecar.istio.io/excludeInboundPorts: "8080,9090"  # 不拦截进入 pod 8080/9090 端口的流量
        traffic.sidecar.istio.io/excludeOutboundPorts: "80,443"    # 不拦截从 pod 出去到其他域名的 80/443 端口的流量
```

## 七、重写 Host 字段的值

仍然假设我们使用了 APISIX/Kong 作为入口网关，那么具体的路由逻辑其实都是已经由它们处理了。

为了在 Istio 上实现统一的流量控制，我们会在 APISIX 上将 Host 直接修改为服务名，这样服务网格就只需要根据服务名来匹配 host 字段，然后进行流量切分、流量镜像等操作即可。

这里就有个问题——有的服务确实需要获取到客户端传递的 Host 请求头，但是上面我们已经将 Host 直接改写为服务名了！
通用的解决方案是：

1. 在 APISIX 侧修改 Host 为服务名的同时，将 `X-Forwarded-Host` 设置为原始的 `Host` 信息，然后再转发给 Istio 服务网格。
2. Istio 服务网格首先根据 Host 的信息匹配 VirtaulService，进行相应的操作
3. 然后 Istio 将 `Host` 的内容再还原为 `X-Forwarded-Host` 中的值
4. Istio 再将请求转发到具体的 Workloads.

其中第三步还原 Host 的逻辑，可以通过如下 EnvoyFilter 实现：

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: host-rewrite
  namespace: xxx
spec:
  workloadSelector:
    labels:
      app: xxx
  configPatches:
  - applyTo: HTTP_ROUTE
    match:
      context: SIDECAR_OUTBOUND
        portNumber: 8080
    patch:
      operation: MERGE
      value:
        route:
          host_rewrite_header: X-Forwarded-Host
```
