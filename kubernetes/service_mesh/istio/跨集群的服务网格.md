# 多集群服务网格

>https://istio.io/latest/docs/setup/install/multicluster/

多集群服务网格能带给我们什么：

- 打通多集群的内部网格，实现跨集群的服务发现、负载均衡、故障注入等功能
- 多集群共用的网关，可以在网关上实现多集群之间的流量切分、或者优先转发到当前可用区/集群，fail-over 到其他集群/可用区
  - Istio 实际上只支持 region/zone/sub-zone 之间的流量切分，多集群之间的流量切分也需要通过这三个标签来实现！我们会在后面的「区域性负载均衡」中详细介绍。

多集群有哪些需要重点关注的东西：

- 有哪几种安装方案，分别有什么优缺点？
- 如何安装，如何平滑地修改、升级？
- 多集群之间的流量管控能力如何？能否实现可用区、地域、集群纬度的流量切分与负载均衡？
- 是否会造成跨区流量？
- 从旧的单集群模式迁移到多集群模式，Istio CR 配置需要做什么更改？
- 如果多集群中的部分集群损坏，apiserver 无响应，会对整个服务网格造成什么影响？如何监控这种情况？如何恢复或者说如何隔离问题集群？

## 零、前置条件

- 多集群之间需要使用相同的 ROOT CA 以实现网络间的相互信任，配置方法见 [Plug in CA Certificates](https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/)
- 多集群之间要能互相访问到对方的 apiserver，因为 istiod 需要访问所有集群的 apiserver 获取 endpoints.

## 一、部署模型

>https://istio.io/latest/docs/ops/deployment/deployment-models/

### 1. Multiple Primary Clusters on The Same Network

在每个 k8s 集群上都安装一个控制平面的副本，这要求

- 所有 k8s 集群都处于同一网络中，不同集群的 pods ip 可以直接互相访问。

此模式下，每个集群的 istiod 都会同时 watch 所有集群的 apiserver 中的 endpoints，但是只从本地 apiserver 中获取 configs.

### 2. Primary and Remote Clusters on The Same Network

即单主+多从结构，主集群跟从集群都可以部署具体的服务。

只有主集群需要安装控制面，此外跨集群的 Gateway 也必须安装在主集群中，这要求：

- 所有 k8s 集群都处于同一网络中，不同集群的 pods ip 可以直接互相访问。

此模式下，主集群的 istiod 会 watch 所有集群的 apiserver 中的 endpoints，但是只从本地 apiserver 中获取 configs.

此部署方式下，主集群跟 Remote 集群都可以部署服务。

为了实现高可用，可以部署多个主集群 + 多个 Remote 集群，跨越多个 Region/Zone 甚至 Cloud.

#### External Control Plane Model

Primary and Remote Clusters 集群的主集群可以运行业务服务，同时包含了控制面+数据面。

而如果主集群仅用于部署 Istio 控制面，不包含数据面，则可称其为 External Control Plane Model，各云厂商的托管版 Istio 服务网格都是使用的这种模型。

部署完成后，各集群包含的 Istio 组件有：

1. 主集群 - 控制平面
   1. 主集群的 Istiod 是控制平面组件，不包含任何数据面组件。
   2. 所有 Istio CRs(CustomResources) 都必须部署在主集群！控制层只会查看主集群中的 configs(Istio CRs)以及远程集群中的 endpoints
   3. 主集群的名字空间被 Istio CRs 的 `hosts` `gateway` 等配置用于查找 k8s Service，因此主集群和子集群建议使用相同的名字空间，或者在 CR 里使用 FQDN
2. 子集群 - 数据平面
   1. istio-sidecar-injector：Sidecar 注入器每个子集群都需要一个，各自处理自己集群的 Sidecar 注入
   5. IngressGateway: 因为主集群仅包含控制面，因此 IngressGateway 需要部署在子集群中，子集群中的 IngressGateway 可以进行跨集群的流量管控。
   2. 为了使服务能在任意集群被访问，需要提前在所有集群部署同一个 Service 配置（DNS 解析 + Endpoints 注册）
   3. 具体的 Deployment/Pod，显然也应该部署在子集群中

为了实现高可用，可以部署多个控制面集群 + 多个 Remote 集群，跨越多个 Region/Zone 甚至 Cloud.

### 3. 跨网络的多集群网络

跨网络的多集群网络，因为多集群之间网络不互通，集群 pods 之间的互访必须通过一层暴露出来的 Gateway 转发。

这种场景应该主要是在跨云/混合云才用得到，暂时略过。


## 二、多集群流量管理

>https://istio.io/latest/docs/ops/configuration/traffic-management/multicluster/

在多集群网络中，跨集群/跨区域的流量的代价会更高，存在额外的跨区流量成本、更高的延迟、更低的吞吐量、更多的网络跳数、更多的中间代理等等缺陷。

比如，假设你希望所有流量都只在集群内部进行负载均衡，可以直接修改全局配置 `MeshConfig.serviceSettings` 然后部署：

```yaml
serviceSettings:
- settings:
    clusterLocal: true
  hosts:
  # 仅此为此服务设置 cluster local 的负载均衡
  - "mysvc.myns.svc.cluster.local"
  # 为 myns 中的所有服务都设置 cluster local 的负载均衡
  - "*.myns.svc.cluster.local"
  # 整个集群内的所有服务，都仅允许在集群内进行负载均衡
  - "*"
```

但是全局设置的方式有点太生硬，一是不方便修改，二是无法设置进行加权负载均衡等更细致的策略。
第二种方式要灵活很多，它是通过 DestinationRule 定义的 subsets 与 VirtualService 的 Match 来实现多集群的负载均衡：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: mysvc-per-cluster-dr
spec:
  host: mysvc.myns.svc.cluster.local
  subsets:
  - name: cluster-1
    labels:
      # 多集群模式下 istio 会在内部为它管理的 Pod 自动添加上 topology.istio.io/cluster 标签
      # 值为安装 istio 时设定的 `values.global.multiCluster.clusterName` 内容
      # https://istio.io/latest/docs/reference/config/labels/
      topology.istio.io/cluster: cluster-1
  - name: cluster-2
    labels:
      topology.istio.io/cluster: cluster-2
---
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: mysvc-cluster-local-vs
spec:
  hosts:
  - mysvc.myns.svc.cluster.local
  http:
  - name: "cluster-1-local"
    match:
    - sourceLabels:
        topology.istio.io/cluster: "cluster-1"
    route:
    - destination:
        host: mysvc.myns.svc.cluster.local
        subset: cluster-1
  - name: "cluster-2-local"
    match:
    - sourceLabels:
        topology.istio.io/cluster: "cluster-2"
    route:
    - destination:
        host: mysvc.myns.svc.cluster.local
        subset: cluster-2
```

第二种方式要更灵活，但是它也存在问题：这种方式把基于 version 的切量逻辑跟基于 cluster 的切量逻辑混合到了一起（比如 `cluster-1-v2`, `cluster-2-v2`），因此请谨慎考虑后再决定要不要使用这种方法。

### 三、区域性负载均衡

>https://istio.io/latest/docs/tasks/traffic-management/locality-load-balancing/

区域性负载均衡功能用于在不同 Region/Zone 之间进行流量控制，主要适用于多集群场景，跟前面提到的多集群流量管控都能实现类似的效果，但是它们处理流量的维度不同，一个是集群维度，一个是 Region/Zone 的纬度。

跟大多数云厂商类似，istio 使用如下三个维度定义一个区域：

- Region: 定义一个大的地理上的地域，比如 AWS 的 `us-east-1` / `us-west-2`，一个 Region 包含有多个 Zones.
  - 在 kubernetes 中节点的标签 `topology.kubernetes.io/region` 标识了它的 region
- Zone: Region 内的一组计算资源，不同的 Zone 之间的网络、电力等关键设施是隔离的，因此大部分故障都只会影响个别 Zone，可通过使用多个 Zones 实现高可用。
  - kubernetes 中使用节点标签 `topology.kubernetes.io/zone` 标识它的 zone
- Sub-Zone: 这是 istio 独有的概念，用于提供更细粒度的区域性流量控制，可以在需要时再研究。
  - Istio 在 kubernetes 中使用自定义标签 `topology.istio.io/subzone` 来标识节点的 sub-zone.

>如果你使用的是云服务商提供的 k8s 集群，所有节点都会被自动打上 `topology.kubernetes.io/region` 跟 `topology.kubernetes.io/zone` 标签。否则的话就需要你自行设定这些标签。

有了如上三个标签后，就可以通过 DestinationRule 的 `spec.trafficPolicy.loadBalancer.localityLbSetting` 来配置区域性负载均衡了~

**Istio 实际上只支持 region/zone/sub-zone 之间的流量切分，多集群之间的流量切分也需要通过这三个标签来实现！**

具体而言要实现多集群之间的流量切分，有如下方法：

- 每个集群的 region+zone 的组合不能相同，这样就可以通过这样一个组合来进行多集群间的流量切分
  - istio 官方文档是使用的这种方式，我看 GCP 貌似也是推荐使用这种方式
- 将 sub-zone 设置为集群名称，这样就能通过此标签实现多集群间的流量切分


以我现有的理解，以及 Istio 官方的举例来看，区域性负载均衡跟 HPA 需要结合使用，因为你调整了不同区域的切量比例后，HPA 肯定也需要进行相应的扩缩容调整。
如果是多集群场景，基本每个集群都会单独部署 HPA 所以没啥毛病。
而官方举例中没说明的单集群场景下，就很可能造成负载不均衡，甚至部分可用区没用实例时可能会直接 503。

因此，单集群场景中应该也可以使用此功能，但是需要为每个 Zone 单独部署一套 Deployment+HPA+PodDiruptionBudget.
