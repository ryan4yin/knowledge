# 配置文件对应的 istioctl 版本：1.12.x
# 适用于生产环境，本地环境可以自行调低资源要求和实例数量
# 这份配置的官方文档：https://istio.io/latest/docs/reference/config/istio.operator.v1alpha1/
# 对应的代码定义：https://github.com/istio/api/blob/1.12.1/mesh/v1alpha1/config.pb.go
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: minimal  # 只部署 istiod 与 CRDs, gateways 全部另行部署
  hub: hub-mirror.c.163.com/istio # 使用 dockerhub 国内镜像仓库（或私有镜像仓库）
  # meshConfig 的所有参数：https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#MeshConfig
  meshConfig:
    # 改为 `/dev/stdout` 可将 access log 输出到容器日志中。但是注意，access log 会非常多，可能影响性能！
    # 另外注意不要把日志输出到文件系统中（如 /tmp/accesslog），因为 sidecar 的文件系统是只读的！
    accessLogFile: ""  # 不收集 access log
    accessLogEncoding: TEXT  # 日志格式，默认为 JSON
    enableEnvoyAccessLogService: false  # 启动 Envoy 的 ALS **服务**，暂时不清楚其含义。不开启也能收集 access log

    # 启用 prometheus 集成，istio 将在数据层注入 prometheus.io 相关注解
    # 注意：prometheus 注解只适用于 prometheus 社区的 helm chart，而 prometheus-operator 只支持 ServiceMonitor 等 CRDs！
    enablePrometheusMerge: true
    enableTracing: false  # 启用链路追踪
    # 启用自动双向认证（默认 true）。
    # 这个有性能损耗，对安全性要求不高的话，一般是建议关掉。
    # (其实很多公司内网都完全不设防的，安全风险最大的是数据库未启用 TLS 协议、以及账号密码保存不当，这个不解决，集群加了 mTLS 也没意义)
    enableAutoMtls: false 
    outboundTrafficPolicy:
      mode: ALLOW_ANY # 允许任意流量出网。改为 REGISTRY_ONLY 将只允许访问已注册的域名。
    # `defaultConfig`: sidecar 和 gateway 的默认配置(envoy)，Pod 上的 `proxy.istio.io/config` 注解可以覆盖此默认值
    # https://istio.io/latest/docs/reference/config/istio.mesh.v1alpha1/#ProxyConfig
    defaultConfig:
      holdApplicationUntilProxyStarts: false  # 等待 Proxy 启动后，再启动 Application 容器
      gatewayTopology:
        numTrustedProxies: 1
        forwardClientCertDetails: 1
    pathNormalization:  # sidecar 和 gateway 对 path 的预处理方式，istio 和 path 有关的所有操作，使用的都是预处理后的 path
      normalization: MERGE_SLASHES  # 自动将多个斜线合并成一个（注意确认，这是否会对服务造成影响。可以先查几天的 access_log，确认下哪些 path 会受到影响，跟对应的业务方确认下）
  components:
    pilot:
      k8s:
        resources: 
          requests:
            cpu: 1000m
            memory: 1024Mi
          limits:
              cpu: 1000m
              memory: 1024Mi
        podDisruptionBudget:
          minAvailable: 60%
        strategy:
          type: RollingUpdate
          rollingUpdate:
            maxSurge: 20%
            maxUnavailable: 0
        nodeSelector:
          eks.amazonaws.com/nodegroup: "network-proxy" # 使用专用的网络节点组，与业务服务隔离开
        affinity:
          podAntiAffinity:  # 使 pod 更分散些
            preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchExpressions:
                  - key: app
                    operator: In
                    values:
                    - istiod
                topologyKey: kubernetes.io/hostname
              weight: 1
  values:
    global: # 全局配置
      configValidation: true
      logAsJson: false  # 日志使用 json 格式
      logging:  # 日志级别
        level: default:info
      istiod:
        enableAnalysis: true  # 在 virtualserivce 的 status 里面显示分析结果。比如一些可能的隐患（host/subset 不存在）
      proxy:
        logLevel: info # sidecar(envoy) 的日志级别
        tracer: zipkin
        privileged: false
        readinessFailureThreshold: 30
        readinessInitialDelaySeconds: 1
        readinessPeriodSeconds: 2
        resources:
          limits:
            cpu: 2000m
            memory: 1024Mi
          requests:
            cpu: 100m
            memory: 128Mi
        lifecycle:
          postStart:  # 即 holdApplicationUntilProxyStarts，因为会被自定义的 lifecycle 覆盖，因此直接写死在这了
            # proxy 作为第一个容器，在它的 postStart 完成前，kubelet 不会启动后续容器~
            exec:
              command:
              - pilot-agent
              - wait
          preStop:  # 等待请求全部处理完后，再关闭 sidecar
            exec:
              command: ["/bin/sh", "-c", "while [ $(netstat -plunt | grep tcp | grep -v envoy | wc -l | xargs) -ne 0 ]; do sleep 1; done"]
      tracer:
        zipkin: # 目前 istio 只支持 zipkin 的协议格式（x-b3-xxx 请求头），不支持 w3c trace-context 标准
          # address: "<jaeger-service>.<jager-space>:9411" # 应该也可以使用外部的 jaeger
    pilot:
      autoscaleEnabled: true
      autoscaleMax: 5
      autoscaleMin: 3
      cpu:
        targetAverageUtilization: 80
    telemetry:
      enabled: true
      # Istio telemetry V2，去掉了 mixer 组件，指标的处理直接在 proxy 上进行
      # istio 使用了两个 envoy 插件来实现直接在数据面生成 k8s service 级别的指标：
      # 1. metadata-exchange 负责在 client/server 两端，互相传递自身的 metadata 信息（两端需要这个信息生成 service 级别的指标，如 istio_requests_total）
      # 2. stats 负责在 envoy 上采集 proxy 级别的流量指标
      v2:
        enabled: true
        metadataExchange:
          # 对于 http 请求，此插件通过注入 HTTP Headers 实现 metadata 交换
          # 而对于 tcp 流量，此插件使用基于 ALPN 的隧道来连接两端的 sidecar，完成 metadata 交换（只有开启了 mTLS 才支持此功能）
          wasmEnabled: false  # 默认情况下 istio 使用二进制编译的插件，这个开关换成实验性的 wasm 模块（代码是同一套，只是加载方式不同）
        prometheus:
          # istio 通过 EnvoyFilter 来配置 proxy 生成 prometheus 指标，这里可以实现自定义指标、增减指标的维度
          enabled: true
          wasmEnabled: false

