# Kubernetes 最佳实践


1. 安全：参见 <security/README.md>
1. 调度策略：设置 Pod 反亲和以实现 Pod 的均匀分布、设置资源请求与限制、设置 HPA


## 一、优雅停止（Gracful Shutdown）与 502/504 报错

如果 Pod 正在处理大量请求（比如 1000 QPS+）时，因为节点故障或「竞价节点」被回收等原因被重新调度，
你可能会观察到在容器被 terminate 的一段时间内出现少量 502/504。

为了搞清楚这个问题，需要先理解清楚 terminate 一个 Pod 的流程：

1. Pod 的状态被设为「Terminating」，（几乎）同时该 Pod 被从所有关联的 Service Endpoints 中移除
2. `preStop` 钩子被执行，它可以是一个命令，或者一个对 Pod 中容器的 http 调用
   1. 如果你的程序在收到 SIGTERM 信号时，无法优雅退出，就可以考虑使用 `preStop`
   2. 如果让程序本身支持优雅退出比较麻烦的话，用 `preStop` 实现优雅退出是一个非常好的方式
3. 将 SIGTERM 发送给 Pod 中的所有容器
4. 继续等待，直到超过 `spec.terminationGracePeriodSeconds` 设定好的时间，这个值默认为 30s
   1. 需要注意的是，这个优雅退出的等待计时是与 `preStop` 同步开始的！而且它也不会等待 `preStop` 结束！
5. 如果超过了 `spec.terminationGracePeriodSeconds` 容器仍然没有停止，k8s 将会发送 SIGKILL 信号给容器
6. 进程全部终止后，整个 Pod 完全被清理掉

**注意**：1 和 2 两个工作是异步发生的，所以可能会出现「Pod 还在 Service Endpoints 中，但是 `preStop` 已经执行了」的情况，我们需要考虑到这种状况的发生。

了解了上面的流程后，我们就能分析出两种错误码出现的原因：

- 502：应用程序在收到 SIGTERM 信号后直接终止了运行，导致部分还没有被处理完的请求直接中断，代理层返回 502 表示这种情况
- 504：Service Endpoints 移除不够及时，在 Pod 已经被终止后，仍然有个别请求被路由到了该 Pod，得不到响应导致 504

通常的解决方案是，在 Pod 的 `preStop` 步骤加一个 15s/30s 的等待时间。
其原理是：在 Pod 处理 terminating 状态的时候，就会被从 Service Endpoints 中移除，也就不会再有新的请求过来了。
在 `preStop` 等待 15s/30s，基本就能保证所有的请求都在容器死掉之前被处理完成（一般来说，绝大部分请求的处理时间都在 300ms 以内吧）。

一个简单的示例如下，它使 Pod 被终止时，总是先等待 30s，再发送 SIGTERM 信号给容器：

```yaml
lifecycle:
  preStop:
    exec:
      command:
      - /bin/sleep
      - "30"
```


### 参考

- [Kubernetes best practices: terminating with grace](https://cloud.google.com/blog/products/containers-kubernetes/kubernetes-best-practices-terminating-with-grace)
- [Graceful shutdown in Kubernetes is not always trivial](https://medium.com/flant-com/kubernetes-graceful-shutdown-nginx-php-fpm-d5ab266963c2)


## 二、[节点维护与Pod干扰预算](https://kubernetes.io/zh/docs/tasks/run-application/configure-pdb/)

在我们通过 `kubectl drain` 将某个节点上的容器驱逐走的时候，
kubernetes 会依据 Pod 的「PodDistruptionBuget」来进行 Pod 的驱逐。

如果不设置任何明确的 PodDistruptionBuget，Pod 将会被直接杀死，然后在别的节点重新调度，**这可能导致服务中断！**

PDB 是一个单独的 CR 自定义资源，示例如下：

```yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: podinfo-pdb
spec:
  # 如果不满足 PDB，Pod 驱逐将会失败！
  minAvailable: 1      # 最少也要维持一个 Pod 可用
#   maxUnavailable: 1  # 最大不可用的 Pod 数
  selector:
    matchLabels:
      app: podinfo
```

如果在进行节点维护时(kubectl drain)，Pod 不满足 PDB，drain 将会失败，示例：

```shell
> kubectl drain node-205 --ignore-daemonsets --delete-local-data
node/node-205 cordoned
WARNING: ignoring DaemonSet-managed Pods: kube-system/calico-node-nfhj7, kube-system/kube-proxy-94dz5
evicting pod default/podinfo-7c84d8c94d-h9brq
evicting pod default/podinfo-7c84d8c94d-gw6qf
error when evicting pod "podinfo-7c84d8c94d-h9brq" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
error when evicting pod "podinfo-7c84d8c94d-h9brq" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
error when evicting pod "podinfo-7c84d8c94d-h9brq" (will retry after 5s): Cannot evict pod as it would violate the pod's disruption budget.
evicting pod default/podinfo-7c84d8c94d-h9brq
pod/podinfo-7c84d8c94d-gw6qf evicted
pod/podinfo-7c84d8c94d-h9brq evicted
node/node-205 evicted
```

上面的示例中，podinfo 一共有两个副本，都运行在 node-205 上面。我给它设置了干扰预算 PDB `minAvailable: 1`。

然后使用 `kubectl drain` 驱逐 Pod 时，其中一个 Pod 被立即驱逐走了，而另一个 Pod 大概在 15 秒内一直驱逐失败。
因为第一个 Pod 还没有在新的节点上启动完成，它不满足干扰预算 PDB `minAvailable: 1` 这个条件。

大约 15 秒后，最先被驱逐走的 Pod 在新节点上启动完成了，另一个 Pod 满足了 PDB 所以终于也被驱逐了。这才完成了一个节点的 drain 操作。

>ClusterAutoscaler 等集群节点伸缩组件，在缩容节点时也会考虑 PodDisruptionBudget. 如果你的集群使用了 ClusterAutoscaler 等动态扩缩容节点的组件，强烈建议设置为所有服务设置 PodDisruptionBudget.

### 最佳实践 Deployment + HPA + PodDisruptionBudget

一般而言，一个服务的每个版本，都应该包含如下三个资源：

- Deployment: 管理服务自身的 Pods 嘛
- HPA: 负责 Pods 的扩缩容，通常使用 CPU 指标进行扩缩容
- PodDisruptionBudget(PDB): 建议按照 HPA 的目标值，来设置 PDB. 
  - 比如 HPA CPU 目标值为 60%，就可以考虑设置 PDB `minAvailable=65%`，保证至少有 65% 的 Pod 可用。这样理论上极限情况下 QPS 均摊到剩下 65% 的 Pods 上也不会造成雪崩（这里假设 QPS 和 CPU 是完全的线性关系） 

## 三、节点亲和性与节点组

我们一个集群，通常会使用不同的标签为节点组进行分类，比如 kubernetes 自动生成的一些节点标签：

- `kubernetes.io/os`: 通常都用 `linux`
- `kubernetes.io/arch`: `amd64`, `arm64`
- `topology.kubernetes.io/region` 和 `topology.kubernetes.io/zone`: 云服务的区域及可用区

如果你使用的是 aws，那 aws 有一些自定义的节点标签：

- `eks.amazonaws.com/nodegroup`: aws eks 节点组的名称，同一个节点组使用同样的 aws ec2 实例模板
  - 比如 arm64 节点组、amd64/x64 节点组
  - 内存比例高的节点组如 m 系实例，计算性能高的节点组如 c 系列
  - 竞价实例节点组：这个省钱啊，但是动态性很高，随时可能被回收
  - 按量付费节点组：这类实例贵，但是稳定。

假设你希望优先选择竞价实例跑你的 Pod，如果竞价实例暂时跑满了，就选择按量付费实例。
那 `nodeSelector` 就满足不了你的需求了，你需要使用 `nodeAffinity`，示例如下:

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: xxx
  namespace: xxx
spec:
  # ...
  template:
    # ...
    spec:
      affinity:
        nodeAffinity:
          # 优先选择 spot-group-c 的节点
          preferredDuringSchedulingIgnoredDuringExecution:
          - preference:
              matchExpressions:
              - key: eks.amazonaws.com/nodegroup
                operator: In
                values:
                - spot-group-c
            weight: 80  # 节点的权重，在有多个 preferred 策略时，被用于确定优先级。
         # 如果没 spot-group-c 可用，也可选择 ondemand-group-c 的节点跑
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: eks.amazonaws.com/nodegroup
                operator: In
                values:
                - spot-group-c
                - ondemand-group-c
      containers:
        # ...
```

