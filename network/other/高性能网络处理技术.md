# 高性能网络处理技术

我们通常接触的 Nginx/HAProxy 都是用户空间的网络代理软件，此外也有 netfilter 上面的 iptables/nftables，它在内核空间直接处理四层数据，但是 netfilter 非常陈旧了，也存在性能问题。

比如 kubernetes 的 kube-proxy 就默认利用 iptables 来设置容器网络，在 pods 数量增多的情况下，iptables 的网络延迟和性能会严重下降。因此 kube-proxy 目前也支持使用 ipvs 进行网络配置。

总的来说，我们有一个结论：现代操作系统内核实现的网络协议栈存在性能问题，在高性能网络数据处理领域，这个问题会变得很突出。

对于一个用户空间的网络处理程序而言，它底层会影响性能的部分主要有：

- 内核态/用户态切换
- 套接字加锁
- 分配 skb

于是出现了一些绕开这些问题点的网络数据处理技术：

- DPDK: 直接从网卡把数据拉到用户空间的程序进行处理，完全绕开内核协议栈
  - DPDK 通常会分配几个专用 CPU 核心进行数据包处理
  - DPDK 被认为是一项过渡性的技术，在高性能网络数据处理领域，网络数据面最终会完全从内核卸载到硬件（SmartNIC）。
- XDP/eBPF: 避免内核态/用户态切换，直接在内核中进行数据处理
  - 由于 eBPF 是一个虚拟机，可以运行任何符合要求的代码，因此它比 netfilter 更加通用和强大
- SmartNIC: 直接在网卡中内置 CPU 进行数据包处理，智能网卡通常都直接支持将 eBPF 字节码编译为其机器码，然后注入到 SmartNIC 中
    - 注入示例：`ip link set dev eth0 xdpoffload obj test.o`，将 test.o 注入到 eth0 智能网卡


基于 DPDK 的高性能四层负载均衡：

- [iqiyi/dpvs](https://github.com/iqiyi/dpvs)

基于 XDP/eBPF 的高性能四层负载均衡：

- [facebookincubator/katran](https://github.com/facebookincubator/katran)
- [cilium](https://github.com/cilium/cilium): 基于 BPF/XDP 实现了 kube-proxy 的功能（针对南北向流量）


## 参考

- [从 Maglev 到 Vortex，揭秘 100G＋线速负载均衡的设计与实现 - UCloud](https://www.infoq.cn/article/Maglev-Vortex/)
- [[译] 为容器时代设计的高级 eBPF 内核特性（FOSDEM, 2021）](http://arthurchiao.art/blog/advanced-bpf-kernel-features-for-container-age-zh/)
- [网络数据面技术是如何内卷起来的](https://zhuanlan.zhihu.com/p/395141110)
